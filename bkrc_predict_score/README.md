# bkrc_predict_score

부도예측모델을 통해 선별된 feature를 통해 안정성 점수를 산출하는 코드입니다.

## 목차
- [개요](#개요)
- [주요 기능](#주요-기능)
- [데이터 처리 플로우](#데이터-처리-플로우)
- [사용 방법](#사용-방법)
- [주요 함수 설명](#주요-함수-설명)
- [Feature 설명](#feature-설명)
- [출력 데이터 구조](#출력-데이터-구조)
- [의존성](#의존성)

## 개요

본 시스템은 국민연금 가입자 데이터를 기반으로 기업의 안정성 점수(BizScore)를 산출합니다. 2022년 데이터를 기준으로 등급 경계(bin_edges)를 정의하고, 이를 활용하여 연도별 기업의 안정성을 평가합니다.

## 주요 기능

1. **등급 경계 생성 (Bin Edges)**: 2022년 1~3월 데이터를 기반으로 각 feature의 등급 경계를 정의
2. **안정성 점수 산출 (BizScore)**: 정의된 등급 경계를 활용하여 월별 기업 안정성 점수 계산
3. **BZ_IDX 연동**: ClickHouse에서 BZ_IDX 데이터를 조회하여 결과와 결합
4. **등급 및 순위 계산**: 업종별, 월별 기준으로 백분위수(Q_tile) 및 순위(RANK) 산출
5. **점수 변동성 분석**: 6개월 이동평균 및 표준편차를 활용한 점수 안정성 평가
6. **ClickHouse 저장**: 최종 결과를 `ATCP_SCORE_DM` 테이블에 저장

## 데이터 처리 플로우

```
1. 데이터 로딩
   - 현재 연도 데이터 (year)
   - 1년 전 데이터 (year-1)
   - 2022년 기준 데이터

2. 등급 경계 생성
   - 2022년 1~3월 데이터로 bin_edges 정의

3. BizScore 계산
   - 1년 전 데이터 점수 계산
   - 현재 연도 데이터 점수 계산
   - 두 데이터 병합

4. BZ_IDX 연동
   - ClickHouse에서 BZ_IDX 데이터 조회
   - BizScore와 병합

5. 통계 지표 생성
   - Q_tile (백분위수) 계산
   - RANK (순위) 계산
   - SCORE_DESC (점수 설명) 생성

6. 최종 저장
   - ClickHouse ATCP_SCORE_DM 테이블에 저장
```

## 사용 방법

```bash
# 특정 연도의 데이터 처리 실행
python run.py 2023

# 예: 2024년 데이터 처리
python run.py 2024
```

### 전제 조건
- `./data/` 디렉토리에 Parquet 형식의 원본 데이터 필요
  - 형식: `newBkrcPensionInfo_atan{YYYY}01_{YYYY}12_12.parquet`
  - 예: `newBkrcPensionInfo_atan202301_202312_12.parquet`
- ClickHouse 접속 권한 필요 (172.16.220.34:29000)

## 주요 함수 설명

### `making_bin_edges(df, f_cols)`
**목적**: 2022년 1~3월 데이터를 기반으로 feature별 등급 경계 생성

- **입력**:
  - `df`: 2022년 전체 데이터
  - `f_cols`: 모델에 사용된 feature 리스트
- **출력**: feature별 등급 경계 딕셔너리 (bin_edges)
- **로직**: 각 feature를 동일한 개수의 구간으로 나누어 등급 경계 설정

### `making_bizscore(df, f_cols, bin_edges, year)`
**목적**: 정의된 등급 경계를 활용하여 월별 BizScore 계산

- **입력**:
  - `df`: 전처리 완료된 데이터
  - `f_cols`: feature 리스트
  - `bin_edges`: 등급 경계 딕셔너리
  - `year`: 점수를 계산할 연도
- **출력**: 월별 기업 안정성 점수 DataFrame
- **로직**:
  1. 1~3월 데이터로 등급별 점수 매핑 테이블 생성
  2. 월별로 각 feature의 등급 부여
  3. 등급별 점수를 합산하여 FINAL_SCORE 계산

### `loadBZIDX(year)`
**목적**: ClickHouse에서 BZ_IDX 관련 데이터 조회

- **입력**: `year` - 조회할 연도
- **출력**: BZ_IDX가 포함된 국민연금 마스터 데이터
- **데이터 범위**: 전년도 1월 ~ 해당 연도 12월

### `main(year)`
**목적**: 전체 데이터 처리 파이프라인 실행

**처리 단계** (12단계):
1. 현재 연도 데이터 로딩
2. 1년 전 데이터 로딩
3. 데이터 병합
4. 점수 범위(bin_edges) 생성
5. BizScore 데이터 생성
6. BizScore 데이터 병합
7. BZ_IDX 데이터 로딩 및 병합
8. Q_tile 및 RANK 칼럼 생성
9. SCORE_DESC 칼럼 생성 (6개월 이동평균 기반)
10. 최종 데이터 필터링
11. 데이터 타입 변환
12. ClickHouse 저장

## Feature 설명

모델에 사용되는 10개의 주요 feature:

| Feature | 설명 |
|---------|------|
| `AVG_NMB_LST_YoY` | 평균 상실자 수 전년 동월 대비 |
| `COUNT_COM_RGNO` | 사업장 등록번호 개수 |
| `COUNT_COM_RGNO_YoY` | 사업장 등록번호 개수 전년 동월 대비 |
| `AVG_NMB_ACQ_YoY` | 평균 취득자 수 전년 동월 대비 |
| `AVG_EMP_FLUX_NET` | 평균 순 고용 변동 |
| `EMP_FLUX_VOL` | 고용 변동 규모 |
| `AVG_NMB_SBS_MoM` | 평균 가입자 수 전월 대비 |
| `AVG_AMT_RATE` | 평균 금액 비율 |
| `동종업계_대비_고용규모` | 동종 업계 대비 고용 규모 |
| `AVG_EMP_FLUX_NET_MoM` | 평균 순 고용 변동 전월 대비 |

## 출력 데이터 구조

### SCORE_DESC (점수 설명)
6개월 이동평균 및 표준편차를 기반으로 4가지 유형으로 분류:

| 조건 | SCORE_DESC |
|------|------------|
| 평균 양수 & 표준편차 상위 50% | 평가 결과를 참고 수준으로 확인하세요 |
| 평균 양수 & 표준편차 하위 50% | 평가 결과가 안정적입니다 |
| 평균 음수 & 표준편차 상위 50% | 평가 결과 판단에 각별한 주의가 필요합니다 |
| 평균 음수 & 표준편차 하위 50% | 평가 결과 판단에 주의하세요 |

#### ⚠️ 표준편차 기준 설정 관련 검토 사항

**현재 적용 방식**:
- `ROLLING6_STD_FINAL_SCORE`의 상위/하위 구분은 **DATE(기준월) 기준**으로 순위를 매겨 결정
- 각 기준월별로 전체 기업의 표준편차를 비교하여 상위 50%, 하위 50%로 분류

**향후 개선 검토 필요**:
- DATE 기준이 아닌 다른 기준(예: 업종별, 규모별) 적용 가능성 검토
- 고정 임계값(threshold) 방식 도입 고려
- 현재 방식은 상대적 평가이므로, 절대적 기준 필요 여부 판단 필요

### 주요 컬럼

- **BZ_IDX**: 사업장 인덱스
- **DATE**: 기준 연월 (YYYYMM)
- **RGNO**: 사업장 등록번호
- **FINAL_SCORE**: 최종 안정성 점수
- **Q_tile**: 백분위수 (0~100)
- **RANK**: 업종별 순위
- **SCORE_DESC**: 점수 안정성 설명

## 의존성

```python
pandas
numpy
clickhouse-driver
```

### 내부 모듈
- `src.scoring`: 점수 계산 관련 함수
- `src.preprocessing`: 데이터 전처리 함수
- `src.db`: ClickHouse 연동 함수

## 주의사항

1. **메모리 관리**: 대용량 데이터 처리 시 `gc.collect()`를 통한 명시적 메모리 해제 수행
2. **데이터 의존성**: 2022년 기준 데이터가 반드시 필요 (bin_edges 생성용)
3. **SCORE_DESC NaN 처리**: 6개월 이동평균 계산을 위해 최소 6개월 데이터 필요
4. **타입 변환**: ClickHouse 저장을 위해 적절한 데이터 타입으로 변환 필수
5. **데이터 정합성 (RGNO 누락 이슈)**:
   - **원인**: npsMaster 테이블 재생성 시 일부 RGNO가 제외될 수 있음
   - **영향**: Parquet 파일 재생성 시 이전 버전 대비 일부 사업장 데이터 누락 가능
   - **대응**: 
     - Parquet 파일 재생성 전 npsMaster 테이블의 RGNO 변경 내역 확인 필요
     - 기존 Parquet와 신규 Parquet의 RGNO 개수 비교 검증 권장
     - 중요 분석의 경우 동일한 npsMaster 버전 기준으로 일관성 유지